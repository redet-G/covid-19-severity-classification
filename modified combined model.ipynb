{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3121,"status":"ok","timestamp":1669403472843,"user":{"displayName":"Redet Getachew","userId":"03999675896311084710"},"user_tz":-180},"id":"ZGTIABBjzXvW"},"outputs":[],"source":["import tensorflow as tf\n","import pandas as pd\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from google.colab import drive\n","import os\n","import numpy as np\n","from tensorflow.keras.applications import ResNet50\n","import os, sys"]},{"cell_type":"code","source":["print(\"hello world\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2onIkZTjM7rZ","executionInfo":{"status":"ok","timestamp":1669403501600,"user_tz":-180,"elapsed":10,"user":{"displayName":"Redet Getachew","userId":"03999675896311084710"}},"outputId":"b5936e52-55d0-4d7e-e462-a6955c299b14"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["hello world\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46134,"status":"ok","timestamp":1659819393364,"user":{"displayName":"Redet Getachew","userId":"03999675896311084710"},"user_tz":-180},"id":"Xt2QZ1iUzhdz","outputId":"76e1c69e-6e2c-42e2-9962-28d1c787bd44"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting keras-cv-attention-models\n","  Downloading keras_cv_attention_models-1.3.0-py3-none-any.whl (502 kB)\n","\u001b[K     |████████████████████████████████| 502 kB 4.0 MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-cv-attention-models) (2.8.2+zzzcolab20220719082949)\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from keras-cv-attention-models) (4.6.0)\n","Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 82.3 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (3.17.3)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (3.1.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (0.26.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (1.1.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (1.14.1)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (2.8.0)\n","Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (2.8.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (0.2.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (1.21.6)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (1.47.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (1.2.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (1.6.3)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (4.1.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (1.1.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (14.0.6)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (0.5.3)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (2.8.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (3.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (57.4.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-cv-attention-models) (1.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-cv-attention-models) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-cv-attention-models) (1.5.2)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (0.6.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (1.35.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (3.4.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (1.8.1)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (3.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-cv-attention-models) (3.2.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->keras-cv-attention-models) (2.7.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->keras-cv-attention-models) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons->keras-cv-attention-models) (3.0.9)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (1.9.0)\n","Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (0.6.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (5.9.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (4.64.0)\n","Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (2.3)\n","Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (0.10.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (0.3.5.1)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras-cv-attention-models) (1.56.4)\n","Installing collected packages: tensorflow-addons, keras-cv-attention-models\n","Successfully installed keras-cv-attention-models-1.3.0 tensorflow-addons-0.17.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/shizuo-kaji/CubicalRipser_3dim\n","  Cloning https://github.com/shizuo-kaji/CubicalRipser_3dim to /tmp/pip-req-build-yc6wmdre\n","  Running command git clone -q https://github.com/shizuo-kaji/CubicalRipser_3dim /tmp/pip-req-build-yc6wmdre\n","  Running command git submodule update --init --recursive -q\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: cripser\n","  Building wheel for cripser (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cripser: filename=cripser-0.0.8-cp37-cp37m-linux_x86_64.whl size=138456 sha256=5e2639865487a536899555e6c331bcc821c32a32a3f29bad7d47c0aae319290d\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-36l1ha12/wheels/7b/8e/b1/bf858a2c6c9b80f3f55e1840fc7752c8dd982ab48f6b337df7\n","Successfully built cripser\n","Installing collected packages: cripser\n","Successfully installed cripser-0.0.8\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gudhi\n","  Downloading gudhi-3.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.3 MB)\n","\u001b[K     |████████████████████████████████| 29.3 MB 453 kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from gudhi) (1.21.6)\n","Installing collected packages: gudhi\n","Successfully installed gudhi-3.5.0\n"]}],"source":["!pip install -U keras-cv-attention-models \n","!pip install git+https://github.com/shizuo-kaji/CubicalRipser_3dim\n","!pip install gudhi\n","\n","from keras_cv_attention_models import coatnet\n","import cripser\n","import gudhi\n","from gudhi import representations"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40878,"status":"ok","timestamp":1659819434218,"user":{"displayName":"Redet Getachew","userId":"03999675896311084710"},"user_tz":-180},"id":"Ia7BAmFmzi6M","outputId":"3ba57c3e-b3d9-4ec6-cb23-845de1b37ffe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive')\n","os.chdir(\"/content/drive/MyDrive/Colab Notebooks\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14943,"status":"ok","timestamp":1669403666884,"user":{"displayName":"Redet Getachew","userId":"03999675896311084710"},"user_tz":-180},"id":"_L9s5r6ezkqx","outputId":"17b061d5-4015-473e-df43-5343ae2ed392"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wandb\n","  Downloading wandb-0.13.5-py2.py3-none-any.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 27.6 MB/s \n","\u001b[?25hCollecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n","\u001b[K     |████████████████████████████████| 168 kB 70.7 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.19.6)\n","Collecting setproctitle\n","  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 61.7 MB/s \n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 1.5 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.11.0-py2.py3-none-any.whl (168 kB)\n","\u001b[K     |████████████████████████████████| 168 kB 74.3 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n","\u001b[K     |████████████████████████████████| 166 kB 75.4 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n","\u001b[K     |████████████████████████████████| 166 kB 76.1 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n","\u001b[K     |████████████████████████████████| 162 kB 77.7 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n","\u001b[K     |████████████████████████████████| 162 kB 63.3 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n","\u001b[K     |████████████████████████████████| 158 kB 81.1 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 79.3 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 79.0 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 82.5 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 80.8 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 81.1 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 80.9 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 78.4 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n","\u001b[K     |████████████████████████████████| 156 kB 79.8 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=e8b97bc23d20959709b8fa1a4ab1bb015ddbfbf8350739ea68229b1eeb4f4a1b\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built pathtools\n","Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n","Successfully installed GitPython-3.1.29 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 wandb-0.13.5\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}],"source":["!pip install wandb\n","!wandb login 300316a5295190e1e55dbc98c9b404940941a598\n","#apikey = 300316a5295190e1e55dbc98c9b404940941a598"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fn1weQqmVV8n"},"outputs":[],"source":["import os\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","\n","def reload_model_weights(model, pretrained_dict, sub_release, pretrained=\"imagenet\", mismatch_class=None, request_resolution=-1, method=\"nearest\"):\n","    if pretrained is None:\n","        return\n","    if isinstance(pretrained, str) and pretrained.endswith(\".h5\"):\n","        print(\">>>> Load pretrained from:\", pretrained)\n","        # model.load_weights(pretrained, by_name=True, skip_mismatch=True)\n","        load_weights_with_mismatch(model, pretrained, mismatch_class, request_resolution, method)\n","        return pretrained\n","\n","    file_hash = pretrained_dict.get(model.name, {}).get(pretrained, None)\n","    if file_hash is None:\n","        print(\">>>> No pretrained available, model will be randomly initialized\")\n","        return None\n","\n","    if isinstance(file_hash, dict):\n","        # file_hash is a dict like {224: \"aa\", 384: \"bb\", 480: \"cc\"}\n","        if request_resolution == -1:\n","            input_height = model.input_shape[1]\n","            if input_height is None:  # input_shape is (None, None, 3)\n","                request_resolution = max(file_hash.keys())\n","            else:\n","                request_resolution = min(file_hash.keys(), key=lambda ii: abs(ii - input_height))\n","        pretrained = \"{}_\".format(request_resolution) + pretrained\n","        file_hash = file_hash[request_resolution]\n","        # print(f\"{request_resolution = }, {pretrained = }, {file_hash = }\")\n","    elif request_resolution == -1:\n","        request_resolution = 224  # Default is 224\n","\n","    pre_url = \"https://github.com/leondgarse/keras_cv_attention_models/releases/download/{}/{}_{}.h5\"\n","    url = pre_url.format(sub_release, model.name, pretrained)\n","    file_name = os.path.basename(url)\n","    try:\n","        pretrained_model = keras.utils.get_file(file_name, url, cache_subdir=\"models\", file_hash=file_hash)\n","    except:\n","        print(\"[Error] will not load weights, url not found or download failed:\", url)\n","        return None\n","    else:\n","        print(\">>>> Load pretrained from:\", pretrained_model)\n","        # model.load_weights(pretrained_model, by_name=True, skip_mismatch=True)\n","        load_weights_with_mismatch(model, pretrained_model, mismatch_class, request_resolution, method)\n","        return pretrained_model\n","\n","\n","def load_weights_with_mismatch(model, weight_file, mismatch_class=None, request_resolution=-1, method=\"nearest\"):\n","    model.load_weights(weight_file, by_name=True, skip_mismatch=True)\n","    input_height, input_width = 244, 244\n","    # if mismatch_class is not None:\n","    if mismatch_class is not None and (request_resolution != input_height or request_resolution != input_width):\n","        try:\n","            import h5py\n","\n","            print(\">>>> Reload mismatched weights: {} -> {}\".format(request_resolution, (input_height, input_width)))\n","            ff = h5py.File(weight_file, mode=\"r\")\n","            weights = ff[\"model_weights\"]\n","            for ii in model.layers:\n","                if isinstance(ii, mismatch_class) and ii.name in weights:\n","                    print(\">>>> Reload layer:\", ii.name)\n","                    ss = weights[ii.name]\n","                    # ss = {ww.decode().split(\"/\")[-1] : tf.convert_to_tensor(ss[ww]) for ww in ss.attrs['weight_names']}\n","                    ss = {ww.decode(\"utf8\") if hasattr(ww, \"decode\") else ww: tf.convert_to_tensor(ss[ww]) for ww in ss.attrs[\"weight_names\"]}\n","                    ss = {kk.split(\"/\")[-1]: vv for kk, vv in ss.items()}\n","                    model.get_layer(ii.name).load_resized_pos_emb(ss, method=method)\n","            ff.close()\n","\n","        # print(\">>>> Reload mismatched PositionalEmbedding weights: {} -> {}\".format(request_resolution, input_shape[0]))\n","        # bb = keras.models.load_model(pretrained_model)\n","        # for ii in model.layers:\n","        #     if isinstance(ii, mismatch_class):\n","        #         print(\">>>> Reload layer:\", ii.name)\n","        #         model.get_layer(ii.name).load_resized_pos_emb(bb.get_layer(ii.name))\n","        except:\n","            print(\"[Error] something went wrong in load_weights_with_mismatch\")\n","            pass\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"owWZ6NQcVa81"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import backend as K\n","\n","BATCH_NORM_DECAY = 0.9\n","BATCH_NORM_EPSILON = 1e-5\n","TF_BATCH_NORM_EPSILON = 0.001\n","LAYER_NORM_EPSILON = 1e-5\n","CONV_KERNEL_INITIALIZER = keras.initializers.VarianceScaling(scale=2.0, mode=\"fan_out\", distribution=\"truncated_normal\")\n","# CONV_KERNEL_INITIALIZER = 'glorot_uniform'\n","\n","from keras_cv_attention_models.attention_layers import (\n","    activation_by_name,\n","    batchnorm_with_activation,\n","    conv2d_no_bias,\n","    depthwise_conv2d_no_bias,\n","    drop_block,\n","    layer_norm,\n","    se_module,\n","    output_block,\n","    MultiHeadRelativePositionalEmbedding,\n","    add_pre_post_process,\n",")\n","# from keras_cv_attention_models.download_and_load import reload_model_weights\n","\n","PRETRAINED_DICT = {\"coatnet0\": {\"imagenet\": {160: \"bc4375d2f03b99ac4252770331f0d22f\", 224: \"29213248739d600cc526c11a79d06775\"}}}\n","\n","\n","def mhsa_with_multi_head_relative_position_embedding(\n","    inputs, num_heads=4, key_dim=0, global_query=None, out_shape=None, out_weight=True, qkv_bias=False, out_bias=False, attn_dropout=0, name=None\n","):\n","    _, hh, ww, cc = inputs.shape\n","    key_dim = key_dim if key_dim > 0 else cc // num_heads\n","    qk_scale = float(1.0 / tf.math.sqrt(tf.cast(key_dim, \"float32\")))\n","    out_shape = cc if out_shape is None or not out_weight else out_shape\n","    qk_out = num_heads * key_dim\n","    # vv_dim = out_shape // num_heads\n","    vv_dim = key_dim\n","\n","    if global_query is not None:\n","        # kv = keras.layers.Dense(qk_out * 2, use_bias=qkv_bias, name=name and name + \"kv\")(inputs)\n","        kv = conv2d_no_bias(inputs, qk_out * 2, kernel_size=1, use_bias=qkv_bias, name=name and name + \"kv_\")\n","        kv = tf.reshape(kv, [-1, kv.shape[1] * kv.shape[2], kv.shape[-1]])\n","        key, value = tf.split(kv, [qk_out, out_shape], axis=-1)\n","        query = global_query\n","    else:\n","        # qkv = keras.layers.Dense(qk_out * 3, use_bias=qkv_bias, name=name and name + \"qkv\")(inputs)\n","        # qkv = conv2d_no_bias(inputs, qk_out * 2 + out_shape, kernel_size=1, name=name and name + \"qkv_\")\n","        qkv = conv2d_no_bias(inputs, qk_out * 3, kernel_size=1, use_bias=qkv_bias, name=name and name + \"qkv_\")\n","        qkv = tf.reshape(qkv, [-1, inputs.shape[1] * inputs.shape[2], qkv.shape[-1]])\n","        query, key, value = tf.split(qkv, [qk_out, qk_out, qk_out], axis=-1)\n","        # print(f\"{query.shape = }, {key.shape = }, {value.shape = }, {num_heads = }, {key_dim = }, {vv_dim = }\")\n","        query = tf.transpose(tf.reshape(query, [-1, query.shape[1], num_heads, key_dim]), [0, 2, 1, 3])  #  [batch, num_heads, hh * ww, key_dim]\n","    key = tf.transpose(tf.reshape(key, [-1, key.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  # [batch, num_heads, key_dim, hh * ww]\n","    value = tf.transpose(tf.reshape(value, [-1, value.shape[1], num_heads, vv_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, vv_dim]\n","\n","    attention_scores = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([query, key]) * qk_scale  # [batch, num_heads, hh * ww, hh * ww]\n","    # print(f\"{query.shape = }, {key.shape = }, {value.shape = }, {attention_scores.shape = }, {hh = }\")\n","    attention_scores = MultiHeadRelativePositionalEmbedding(with_cls_token=False, attn_height=hh, name=name and name + \"pos_emb\")(attention_scores)\n","    attention_scores = keras.layers.Softmax(axis=-1, name=name and name + \"attention_scores\")(attention_scores)\n","    attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + \"attn_drop\")(attention_scores) if attn_dropout > 0 else attention_scores\n","\n","    # value = [batch, num_heads, hh * ww, vv_dim], attention_output = [batch, num_heads, hh * ww, vv_dim]\n","    attention_output = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([attention_scores, value])\n","    attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n","    attention_output = tf.reshape(attention_output, [-1, inputs.shape[1], inputs.shape[2], num_heads * vv_dim])\n","    # print(f\">>>> {attention_output.shape = }, {attention_scores.shape = }\")\n","\n","    if out_weight:\n","        # [batch, hh, ww, num_heads * vv_dim] * [num_heads * vv_dim, out] --> [batch, hh, ww, out]\n","        attention_output = keras.layers.Dense(out_shape, use_bias=out_bias, name=name and name + \"output\")(attention_output)\n","    # attention_output = keras.layers.Dropout(output_dropout, name=name and name + \"out_drop\")(attention_output) if output_dropout > 0 else attention_output\n","    return attention_output\n","\n","\n","def res_MBConv(\n","    inputs,\n","    output_channel,\n","    conv_short_cut=True,\n","    strides=1,\n","    expansion=4,\n","    se_ratio=0,\n","    drop_rate=0,\n","    use_dw_strides=True,\n","    bn_act_first=False,\n","    activation=\"gelu\",\n","    name=\"\",\n","):\n","    \"\"\"x ← Proj(Pool(x)) + Conv (DepthConv (Conv (Norm(x), stride = 2))))\"\"\"\n","    preact = batchnorm_with_activation(inputs, activation=None, zero_gamma=False, name=name + \"preact_\")\n","\n","    if conv_short_cut:\n","        shortcut = keras.layers.MaxPool2D(strides, strides=strides, padding=\"SAME\", name=name + \"shortcut_pool\")(inputs) if strides > 1 else inputs\n","        shortcut = conv2d_no_bias(shortcut, output_channel, 1, strides=1, name=name + \"shortcut_\")\n","        # shortcut = batchnorm_with_activation(shortcut, activation=activation, zero_gamma=False, name=name + \"shortcut_\")\n","    else:\n","        shortcut = inputs\n","\n","    # MBConv\n","    input_channel = inputs.shape[-1]\n","    conv_strides, dw_strides = (1, strides) if use_dw_strides else (strides, 1)  # May swap stirdes with DW\n","    nn = conv2d_no_bias(preact, input_channel * expansion, 1, strides=conv_strides, use_bias=bn_act_first, padding=\"same\", name=name + \"expand_\")\n","    nn = batchnorm_with_activation(nn, activation=activation, act_first=bn_act_first, name=name + \"expand_\")\n","    nn = depthwise_conv2d_no_bias(nn, 3, strides=dw_strides, use_bias=bn_act_first, padding=\"same\", name=name + \"MB_\")\n","    nn = batchnorm_with_activation(nn, activation=activation, act_first=bn_act_first, zero_gamma=False, name=name + \"MB_dw_\")\n","    if se_ratio:\n","        nn = se_module(nn, se_ratio=se_ratio / expansion, activation=activation, name=name + \"se_\")\n","    nn = conv2d_no_bias(nn, output_channel, 1, strides=1, padding=\"same\", name=name + \"MB_pw_\")\n","    # nn = batchnorm_with_activation(nn, activation=None, zero_gamma=True, name=name + \"MB_pw_\")\n","    nn = drop_block(nn, drop_rate=drop_rate, name=name)\n","    return keras.layers.Add(name=name + \"output\")([shortcut, nn])\n","\n","\n","def res_ffn(inputs, expansion=4, kernel_size=1, drop_rate=0, activation=\"gelu\", name=\"\"):\n","    \"\"\"x ← x + Module (Norm(x)), similar with typical MLP block\"\"\"\n","    # preact = batchnorm_with_activation(inputs, activation=None, zero_gamma=False, name=name + \"preact_\")\n","    preact = layer_norm(inputs, name=name + \"preact_\")\n","\n","    input_channel = inputs.shape[-1]\n","    nn = conv2d_no_bias(preact, input_channel * expansion, kernel_size, name=name + \"1_\")\n","    nn = activation_by_name(nn, activation=activation, name=name)\n","    nn = conv2d_no_bias(nn, input_channel, kernel_size, name=name + \"2_\")\n","    nn = drop_block(nn, drop_rate=drop_rate, name=name)\n","    # return keras.layers.Add(name=name + \"output\")([preact, nn])\n","    return keras.layers.Add(name=name + \"output\")([inputs, nn])\n","\n","\n","def res_mhsa(inputs, output_channel, conv_short_cut=True, strides=1, head_dimension=32, drop_rate=0, activation=\"gelu\", name=\"\"):\n","    \"\"\"x ← Proj(Pool(x)) + Attention (Pool(Norm(x)))\"\"\"\n","    # preact = batchnorm_with_activation(inputs, activation=None, zero_gamma=False, name=name + \"preact_\")\n","    preact = layer_norm(inputs, name=name + \"preact_\")\n","\n","    if conv_short_cut:\n","        shortcut = keras.layers.MaxPool2D(strides, strides=strides, padding=\"SAME\", name=name + \"shortcut_pool\")(inputs) if strides > 1 else inputs\n","        shortcut = conv2d_no_bias(shortcut, output_channel, 1, strides=1, name=name + \"shortcut_\")\n","        # shortcut = batchnorm_with_activation(shortcut, activation=activation, zero_gamma=False, name=name + \"shortcut_\")\n","    else:\n","        shortcut = inputs\n","\n","    nn = preact\n","    if strides != 1:  # Downsample\n","        # nn = keras.layers.ZeroPadding2D(padding=1, name=name + \"pad\")(nn)\n","        nn = keras.layers.MaxPool2D(pool_size=2, strides=strides, padding=\"SAME\", name=name + \"pool\")(nn)\n","    num_heads = nn.shape[-1] // head_dimension\n","    nn = mhsa_with_multi_head_relative_position_embedding(nn, num_heads=num_heads, key_dim=head_dimension, out_shape=output_channel, name=name + \"mhsa_\")\n","    nn = drop_block(nn, drop_rate=drop_rate, name=name)\n","    # print(f\"{name = }, {inputs.shape = }, {shortcut.shape = }, {nn.shape = }\")\n","    return keras.layers.Add(name=name + \"output\")([shortcut, nn])\n","\n","\n","def CoAtNet(\n","    num_blocks,\n","    out_channels,\n","    stem_width=64,\n","    block_types=[\"conv\", \"conv\", \"transform\", \"transform\"],\n","    strides=[2, 2, 2, 2],\n","    expansion=4,\n","    se_ratio=0.25,\n","    head_dimension=32,\n","    use_dw_strides=True,\n","    bn_act_first=False,  # Experiment, use activation -> BatchNorm instead of BatchNorm -> activation, also set use_bias=True for pre Conv2D layer\n","    input_shape=(224, 224, 3),\n","    num_classes=1000,\n","    activation=\"gelu\",\n","    drop_connect_rate=0,\n","    classifier_activation=\"softmax\",\n","    dropout=0,\n","    pretrained=None,\n","    model_name=\"coatnet\",\n","    kwargs=None,\n","):\n","    img_inputs = keras.layers.Input(input_shape, name=\"image\")\n","    nn = keras.layers.Rescaling(1.0 /(255) )(img_inputs)\n","    \"\"\" stage 0, Stem_stage \"\"\"\n","    nn = conv2d_no_bias(nn, stem_width, 3, strides=2, use_bias=bn_act_first, padding=\"same\", name=\"stem_1_\")\n","    nn = batchnorm_with_activation(nn, activation=activation, act_first=bn_act_first, name=\"stem_1_\")\n","    nn = conv2d_no_bias(nn, stem_width, 3, strides=1, use_bias=bn_act_first, padding=\"same\", name=\"stem_2_\")\n","    # nn = batchnorm_with_activation(nn, activation=activation, name=\"stem_2_\")\n","\n","    \"\"\" stage [1, 2, 3, 4] \"\"\"\n","    total_blocks = sum(num_blocks)\n","    global_block_id = 0\n","    for stack_id, (num_block, out_channel, block_type) in enumerate(zip(num_blocks, out_channels, block_types)):\n","        is_conv_block = True if block_type[0].lower() == \"c\" else False\n","        stack_se_ratio = se_ratio[stack_id] if isinstance(se_ratio, (list, tuple)) else se_ratio\n","        stack_strides = strides[stack_id] if isinstance(strides, (list, tuple)) else strides\n","        for block_id in range(num_block):\n","            name = \"stack_{}_block_{}_\".format(stack_id + 1, block_id + 1)\n","            stride = stack_strides if block_id == 0 else 1\n","            conv_short_cut = True if block_id == 0 else False\n","            block_se_ratio = stack_se_ratio[block_id] if isinstance(stack_se_ratio, (list, tuple)) else stack_se_ratio\n","            block_drop_rate = drop_connect_rate * global_block_id / total_blocks\n","            global_block_id += 1\n","            if is_conv_block:\n","                nn = res_MBConv(\n","                    nn, out_channel, conv_short_cut, stride, expansion, block_se_ratio, block_drop_rate, use_dw_strides, bn_act_first, activation, name=name\n","                )\n","            else:\n","                nn = res_mhsa(nn, out_channel, conv_short_cut, stride, head_dimension, block_drop_rate, activation=activation, name=name)\n","                nn = res_ffn(nn, expansion=expansion, drop_rate=block_drop_rate, activation=activation, name=name + \"ffn_\")\n","\n","    # nn = output_block(nn, num_classes=num_classes, drop_rate=dropout, classifier_activation=classifier_activation, act_first=bn_act_first)\n","    # model = keras.models.Model(inputs, nn, name=model_name)\n","    # add_pre_post_process(model, rescale_mode=\"torch\")\n","    # reload_model_weights(model, PRETRAINED_DICT, \"coatnet\", pretrained, MultiHeadRelativePositionalEmbedding)\n","    nn = keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")(nn)\n","    nn = keras.layers.Dense(256, activation='relu')(nn)\n","    ## add the model \n","    ph_inputs = keras.Input(shape=(512,), name=\"bettiCurve\")\n","    \n","    ph_features = keras.layers.Rescaling(1.0 /(25000) )(ph_inputs)\n","    ph_features = keras.layers.Flatten()(ph_features)\n","    ph_features = keras.layers.Dense(256*2, activation='relu')(ph_features)\n","    for i in range(8):\n","      ph_features = keras.layers.Dense(256, activation='relu')(ph_features)\n","    \n","    combined_features = keras.layers.concatenate([ph_features, nn],axis=1)\n","\n","    nn = combined_features\n","    if dropout > 0:\n","        nn = keras.layers.Dropout(dropout, name=\"head_drop\")(nn)\n","    # outputs = keras.layers.Dense(num_classes, dtype=\"float32\",  kernel_regularizer=tf.keras.regularizers.l2(0.01), activation=classifier_activation, name=\"predictions\")(nn)\n","    outputs = keras.layers.Dense(num_classes, dtype=\"float32\", activation=classifier_activation, name=\"predictions\")(nn)\n","\n","    combined_model = keras.Model(\n","        inputs=[img_inputs,ph_inputs],\n","        outputs=outputs,\n","        name=model_name\n","    )\n","    \n","    # add_pre_post_process(combined_model, rescale_mode=\"torch\")\n","    reload_model_weights(combined_model, PRETRAINED_DICT, \"coatnet\", pretrained, MultiHeadRelativePositionalEmbedding, request_resolution = 224)\n","    \n","    return combined_model\n","\n","\n","def CoAtNetT(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0, classifier_activation=\"softmax\", **kwargs):\n","    num_blocks = [3, 4, 6, 3]\n","    out_channels = [64, 128, 256, 512]\n","    stem_width = 64\n","    return CoAtNet(**locals(), model_name=\"coatnett\", **kwargs)\n","\n","\n","def CoAtNet0(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0, classifier_activation=\"softmax\", pretrained=\"imagenet\", **kwargs):\n","    num_blocks = [1, 1, 2,1]\n","    out_channels = [96, 192, 384, 768]\n","    stem_width = 64\n","    return CoAtNet(**locals(), model_name=\"coatnet0\", **kwargs)\n","\n","\n","def CoAtNet1(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0.3, classifier_activation=\"softmax\", **kwargs):\n","    num_blocks = [2, 6, 14, 2]\n","    out_channels = [96, 192, 384, 768]\n","    stem_width = 64\n","    return CoAtNet(**locals(), model_name=\"coatnet1\", **kwargs)\n","\n","\n","def CoAtNet2(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0.5, classifier_activation=\"softmax\", **kwargs):\n","    num_blocks = [2, 6, 14, 2]\n","    out_channels = [128, 256, 512, 1024]\n","    stem_width = 128\n","    return CoAtNet(**locals(), model_name=\"coatnet2\", **kwargs)\n","\n","\n","def CoAtNet3(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0.7, classifier_activation=\"softmax\", **kwargs):\n","    num_blocks = [2, 6, 14, 2]\n","    out_channels = [192, 384, 768, 1536]\n","    stem_width = 192\n","    return CoAtNet(**locals(), model_name=\"coatnet3\", **kwargs)\n","\n","\n","def CoAtNet4(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0.2, classifier_activation=\"softmax\", **kwargs):\n","    num_blocks = [2, 12, 28, 2]\n","    out_channels = [192, 384, 768, 1536]\n","    stem_width = 192\n","    return CoAtNet(**locals(), model_name=\"coatnet4\", **kwargs)\n","\n","\n","def CoAtNet5(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0.2, classifier_activation=\"softmax\", **kwargs):\n","    num_blocks = [2, 12, 28, 2]\n","    out_channels = [256, 512, 1280, 2048]\n","    stem_width = 192\n","    head_dimension = 64\n","    return CoAtNet(**locals(), model_name=\"coatnet5\", **kwargs)\n","\n","\n","def CoAtNet6(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0.2, classifier_activation=\"softmax\", **kwargs):\n","    num_blocks = [2, 4, 8, 42, 2]\n","    out_channels = [192, 384, 768, 1536, 2048]\n","    block_types = [\"conv\", \"conv\", \"conv\", \"transfrom\", \"transform\"]\n","    strides = [2, 2, 2, 1, 2]\n","    stem_width = 192\n","    head_dimension = 128\n","    return CoAtNet(**locals(), model_name=\"coatnet6\", **kwargs)\n","\n","\n","def CoAtNet7(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0.2, classifier_activation=\"softmax\", **kwargs):\n","    num_blocks = [2, 4, 8, 42, 2]\n","    out_channels = [256, 512, 1024, 2048, 3072]\n","    block_types = [\"conv\", \"conv\", \"conv\", \"transfrom\", \"transform\"]\n","    strides = [2, 2, 2, 1, 2]\n","    stem_width = 192\n","    head_dimension = 128\n","    return CoAtNet(**locals(), model_name=\"coatnet7\", **kwargs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kRw_wyx7zmZi"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11536,"status":"ok","timestamp":1659819461155,"user":{"displayName":"Redet Getachew","userId":"03999675896311084710"},"user_tz":-180},"id":"rPmn2BQ-zorI","outputId":"06d34430-3a7f-4bf9-8d4c-298637dda4f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 0.9108126721763086, 1: 0.6654088050314465, 2: 2.5047348484848486}\n"]}],"source":["from glob import glob\n","data_dir = \"combined_segmented_cropped_modified_resized_subfolder_80_20/train\"\n","test_dir = \"combined_segmented_cropped_modified_resized_subfolder_80_20/val\"\n","mild = len(glob(os.path.join(data_dir+\"/mild\", \"*.jpg\")))\n","moderate = len(glob(os.path.join(data_dir+\"/moderate\", \"*.jpg\")))\n","severe = len(glob(os.path.join(data_dir+\"/severe\", \"*.jpg\")))\n","\n","total  = mild+moderate+severe\n","weight_for_0 = (1 / mild) * (total / 3.0)\n","weight_for_1 = (1 / moderate) * (total / 3.0)\n","weight_for_2 = (1 / severe) * (total / 3.0)\n","class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n","print(class_weight)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":543026,"status":"ok","timestamp":1659820004160,"user":{"displayName":"Redet Getachew","userId":"03999675896311084710"},"user_tz":-180},"id":"FuCeQN0KTWVJ","outputId":"c0d5ba40-ad91-4749-9b9d-72ec94b4807f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2645 files belonging to 3 classes.\n","Found 664 files belonging to 3 classes.\n"]}],"source":["import matplotlib.pyplot as plt\n","\n","def calculate(input):\n","  pd = cripser.computePH(input,maxdim=1,location=\"birth\")\n","  pds0_bd =np.array(pd[pd[:,0] == 0,1:3])\n","  pds1_bd =np.array(pd[pd[:,0] == 1,1:3])\n","\n","  pds0_bd=np.clip(pds0_bd,0,255)\n","  pds1_bd=np.clip(pds1_bd,0,255)\n","  curves = representations.vector_methods.BettiCurve(256,sample_range = [0,255])\n","  result = np.array(curves(pds0_bd))\n","  result1 = np.array(curves(pds1_bd))\n","  return np.concatenate([result, result1],0)\n","\n","image_size = (224, 224)\n","\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    data_dir,\n","    seed=1337,\n","    image_size=image_size,\n","    batch_size=16,\n","    color_mode='rgb',\n","    label_mode='categorical',\n",")\n","val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    test_dir,\n","    seed=1337,\n","    image_size=image_size,\n","    batch_size=16,\n","    color_mode='rgb',\n","    label_mode='categorical',\n",")\n","data_augmentation = keras.Sequential(\n","    [\n","        layers.RandomFlip(\"horizontal\"),\n","        layers.RandomRotation(0.05),\n","        # layers.RandomZoom(height_factor=(0.2, 0.3))\n","    ]\n",")\n","# train_ds = train_ds.prefetch(buffer_size=wandb.config['batch_size'])\n","# val_ds = val_ds.prefetch(buffer_size=wandb.config['batch_size'])\n","# augmented_train_ds = train_ds.prefetch(buffer_size=wandb.config['batch_size'])\n","\n","augmented_train_ds = train_ds.map(\n","  lambda x, y: (data_augmentation(x, training=True), y))\n","\n","\n","train_ds_source = train_ds.unbatch()\n","val_ds_source = val_ds.unbatch()\n","augmented_val_ds = augmented_train_ds.unbatch()\n","\n","ph_train_data = [];\n","ph_train_label = [];\n","image_train_data = [];\n","image_train_label = [];\n","for data,label in train_ds_source:\n","  ph_train_data.append(calculate(data[:,:,0]))\n","  ph_train_label.append(label)\n","  image_train_data.append(data)\n","  image_train_label.append(label)\n","\n","ph_val_data = [];\n","ph_val_label = [];\n","image_val_data = [];\n","image_val_label = [];\n","for data,label in val_ds_source:\n","  ph_val_data.append(calculate(data[:,:,0]))\n","  ph_val_label.append(label)\n","  image_val_data.append(data)\n","  image_val_label.append(label)\n","\n","tf.random.set_seed(8412)\n","ph_train_data=tf.random.shuffle(ph_train_data, seed=8412)\n","tf.random.set_seed(8412)\n","ph_train_label=tf.random.shuffle(ph_train_label, seed=8412)\n","tf.random.set_seed(8412)\n","image_train_data=tf.random.shuffle(image_train_data, seed=8412)\n","tf.random.set_seed(8412)\n","image_train_label=tf.random.shuffle(image_train_label, seed=8412)\n","\n","ph_train_data=np.array(ph_train_data)\n","ph_train_label=np.array(ph_train_label)\n","\n","ph_val_data=np.array(ph_val_data)\n","ph_val_label=np.array(ph_val_label)\n","\n","image_train_data=np.array(image_train_data)\n","image_train_label=np.array(image_train_label)\n","\n","image_val_data=np.array(image_val_data)\n","image_val_label=np.array(image_val_label)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EG4nSRfs0ChY"},"outputs":[],"source":["\n","\n","# ph_train_ds = tf.data.Dataset.from_tensor_slices(ph_train_data)\n","# ph_val_ds = tf.data.Dataset.from_tensor_slices(ph_val_data)\n","# ph_train_label_ds = tf.data.Dataset.from_tensor_slices(ph_train_label)\n","\n","# combined_train_ds = tf.data.Dataset.from_tensor_slices({\"image\": image_train_data,\"bettiCurve\":ph_train_ds},ph_train_label_ds)\n","\n","# ph_train_label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HkjZlfPGIyd7"},"outputs":[],"source":["# assert(np.array_equal(image_train_label,ph_train_label))\n","train_data = {\"image\": image_train_data,\"bettiCurve\":ph_train_data}\n","val_data = ([image_val_data, ph_val_data], ph_val_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DCUFizEYzqFq"},"outputs":[],"source":["\n","\n","# wandb.config = {\n","#   \"learning_rate\": 0.0001,\n","#   \"epochs\": 10,\n","#   \"batch_size\": 16\n","# }\n","# os.chdir(\"/content/drive/MyDrive/Colab Notebooks\")\n","\n","def make_model(input_shape, num_classes,dropout=None,config=None):\n","\n","    # base_model = coatnet.CoAtNet(input_shape=input_shape, num_classes=num_classes, drop_connect_rate=config['drop_connect_rate'] , block_types=[\"conv\", \"conv\", \"transform\"], num_blocks = [wandb.config['num_layer_1'], wandb.config['num_layer_2'], wandb.config['num_layer_3']], out_channels = [32 ,64, 128],stem_width = 64);\n","    base_model = CoAtNet0(input_shape,num_classes,dropout=dropout)\n","    return base_model;\n","\n","def train():\n","  \n","  wandb.init()\n","  # data_dir = \"data\"\n","  input_shape=image_size + (3,)\n","  num_classes=3\n","  # model = coatnet.CoAtNet(input_shape=input_shape, num_classes=num_classes, \n","  #                         drop_connect_rate=wandb.config['drop_connect_rate'] , \n","  #                         block_types=[\"conv\", \"conv\", \"transform\"], \n","  #                         num_blocks = [2, 2, 2], \n","  #                         out_channels = [32 ,64, 128],\n","  #                         stem_width = 64);\n","  \n","  model = make_model(input_shape,num_classes,dropout=wandb.config['dropout'])\n","\n","  # keras.utils.plot_model(model, show_shapes=True)\n","  #suffle and handel randomization\n","\n","  callbacks = [\n","      WandbCallback(),\n","  ]\n","\n","  if wandb.config['optimizer']=='adam':\n","      optimizer = tf.keras.optimizers.Adam(wandb.config['learning_rate'])\n","  else:\n","      optimizer = tf.keras.optimizers.SGD(wandb.config['learning_rate'])\n","\n","  model.compile(\n","      optimizer=optimizer,\n","      loss=\"categorical_crossentropy\",\n","      metrics=[\"accuracy\",\n","              tf.keras.metrics.SensitivityAtSpecificity(0.5,class_id=0,name=\"Sensitivity_Mild\"),\n","              tf.keras.metrics.SensitivityAtSpecificity(0.5,class_id=1,name=\"Sensitivity_Moderate\"),\n","              tf.keras.metrics.SensitivityAtSpecificity(0.5,class_id=2,name=\"Sensitivity_Severe\"),\n","              ],\n","      run_eagerly=True\n","  )\n","\n","  model.fit(\n","          train_data,ph_train_label, epochs=wandb.config['epochs'], batch_size=wandb.config['batch_size'], callbacks=callbacks,validation_data=val_data,\n","      )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vu5XMZiIc4Gn"},"outputs":[],"source":["# keras.utils.plot_model(model, show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["f84e4bf184c44e99a5196b1d9bfda5b9","e00aafc1ad1645e7bf90c7cc61f788fe","717fd1544ab4453587d8b857ef82659b","8116d8e817fd485bbc5c424fd3683a1f","f02c4cb4893a4a7b95ba20009af8a86c","b616f0d79a74465d98a8d4cc420eafab","202c074cdcd144f2b155323e10d16079","2f572ea4327a4ea6843e1c87331078e6"]},"id":"hMHIEm8_zwpY","outputId":"6a460b85-99fa-48e7-a81f-e1a7a756884b"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yaw62ovw with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 5\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.33614933621752047\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 19\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0007206382089706126\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mredet\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.13.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/sample_data/wandb/run-20220806_210650-yaw62ovw</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/redet/final-project-results/runs/yaw62ovw\" target=\"_blank\">comic-sweep-9</a></strong> to <a href=\"https://wandb.ai/redet/final-project-results\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/redet/final-project-results/sweeps/3pmd91nk\" target=\"_blank\">https://wandb.ai/redet/final-project-results/sweeps/3pmd91nk</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading data from https://github.com/leondgarse/keras_cv_attention_models/releases/download/coatnet/coatnet0_224_imagenet.h5\n","93765632/93762208 [==============================] - 9s 0us/step\n","93773824/93762208 [==============================] - 9s 0us/step\n",">>>> Load pretrained from: /root/.keras/models/coatnet0_224_imagenet.h5\n","WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (512, 3). Received saved weight with shape (768, 1000)\n","WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (3,). Received saved weight with shape (1000,)\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"]},{"output_type":"stream","name":"stdout","text":[">>>> Reload mismatched weights: 224 -> (244, 244)\n",">>>> Reload layer: stack_3_block_1_mhsa_pos_emb\n",">>>> Reload layer: stack_3_block_2_mhsa_pos_emb\n",">>>> Reload layer: stack_4_block_1_mhsa_pos_emb\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/internal/flops_registry.py:138: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n","Epoch 1/19\n","529/529 [==============================] - ETA: 0s - loss: 1.0924 - accuracy: 0.4647 - Sensitivity_Mild: 0.5661 - Sensitivity_Moderate: 0.4815 - Sensitivity_Severe: 0.6392"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `_wrapped_model` contains input name(s) bettiCurve with unsupported characters which will be renamed to betticurve in the SavedModel.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_210650-yaw62ovw/files/model-best/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_210650-yaw62ovw/files/model-best/assets\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/sample_data/wandb/run-20220806_210650-yaw62ovw/files/model-best)... Done. 1.3s\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r529/529 [==============================] - 111s 188ms/step - loss: 1.0924 - accuracy: 0.4647 - Sensitivity_Mild: 0.5661 - Sensitivity_Moderate: 0.4815 - Sensitivity_Severe: 0.6392 - val_loss: 1.0198 - val_accuracy: 0.4593 - val_Sensitivity_Mild: 0.7901 - val_Sensitivity_Moderate: 0.6988 - val_Sensitivity_Severe: 0.8876\n","Epoch 2/19\n","529/529 [==============================] - ETA: 0s - loss: 0.9172 - accuracy: 0.5577 - Sensitivity_Mild: 0.7676 - Sensitivity_Moderate: 0.5698 - Sensitivity_Severe: 0.8267"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `_wrapped_model` contains input name(s) bettiCurve with unsupported characters which will be renamed to betticurve in the SavedModel.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_210650-yaw62ovw/files/model-best/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_210650-yaw62ovw/files/model-best/assets\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/sample_data/wandb/run-20220806_210650-yaw62ovw/files/model-best)... Done. 0.9s\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r529/529 [==============================] - 100s 188ms/step - loss: 0.9172 - accuracy: 0.5577 - Sensitivity_Mild: 0.7676 - Sensitivity_Moderate: 0.5698 - Sensitivity_Severe: 0.8267 - val_loss: 0.9170 - val_accuracy: 0.5211 - val_Sensitivity_Mild: 0.9547 - val_Sensitivity_Moderate: 0.5994 - val_Sensitivity_Severe: 0.9101\n","Epoch 3/19\n","529/529 [==============================] - ETA: 0s - loss: 0.8347 - accuracy: 0.5989 - Sensitivity_Mild: 0.8667 - Sensitivity_Moderate: 0.6921 - Sensitivity_Severe: 0.9261"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `_wrapped_model` contains input name(s) bettiCurve with unsupported characters which will be renamed to betticurve in the SavedModel.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_210650-yaw62ovw/files/model-best/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_210650-yaw62ovw/files/model-best/assets\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/sample_data/wandb/run-20220806_210650-yaw62ovw/files/model-best)... Done. 1.1s\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r529/529 [==============================] - 100s 189ms/step - loss: 0.8347 - accuracy: 0.5989 - Sensitivity_Mild: 0.8667 - Sensitivity_Moderate: 0.6921 - Sensitivity_Severe: 0.9261 - val_loss: 0.7219 - val_accuracy: 0.6943 - val_Sensitivity_Mild: 0.9424 - val_Sensitivity_Moderate: 0.8795 - val_Sensitivity_Severe: 0.9663\n","Epoch 4/19\n","529/529 [==============================] - 89s 169ms/step - loss: 0.8228 - accuracy: 0.6163 - Sensitivity_Mild: 0.8678 - Sensitivity_Moderate: 0.7442 - Sensitivity_Severe: 0.9176 - val_loss: 0.7326 - val_accuracy: 0.6611 - val_Sensitivity_Mild: 0.9383 - val_Sensitivity_Moderate: 0.8434 - val_Sensitivity_Severe: 0.9438\n","Epoch 5/19\n","529/529 [==============================] - 89s 168ms/step - loss: 0.8055 - accuracy: 0.6250 - Sensitivity_Mild: 0.8771 - Sensitivity_Moderate: 0.7396 - Sensitivity_Severe: 0.9489 - val_loss: 0.7400 - val_accuracy: 0.6687 - val_Sensitivity_Mild: 0.9259 - val_Sensitivity_Moderate: 0.8645 - val_Sensitivity_Severe: 0.9663\n","Epoch 6/19\n","529/529 [==============================] - 89s 169ms/step - loss: 0.7974 - accuracy: 0.6223 - Sensitivity_Mild: 0.8905 - Sensitivity_Moderate: 0.7562 - Sensitivity_Severe: 0.9318 - val_loss: 0.7641 - val_accuracy: 0.6069 - val_Sensitivity_Mild: 0.9218 - val_Sensitivity_Moderate: 0.8404 - val_Sensitivity_Severe: 0.9888\n","Epoch 7/19\n","529/529 [==============================] - 89s 169ms/step - loss: 0.7924 - accuracy: 0.6212 - Sensitivity_Mild: 0.8905 - Sensitivity_Moderate: 0.7358 - Sensitivity_Severe: 0.9205 - val_loss: 0.8236 - val_accuracy: 0.6054 - val_Sensitivity_Mild: 0.8807 - val_Sensitivity_Moderate: 0.7861 - val_Sensitivity_Severe: 0.9326\n","Epoch 8/19\n","529/529 [==============================] - ETA: 0s - loss: 0.7960 - accuracy: 0.6287 - Sensitivity_Mild: 0.9008 - Sensitivity_Moderate: 0.7517 - Sensitivity_Severe: 0.9403"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `_wrapped_model` contains input name(s) bettiCurve with unsupported characters which will be renamed to betticurve in the SavedModel.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_210650-yaw62ovw/files/model-best/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_210650-yaw62ovw/files/model-best/assets\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/sample_data/wandb/run-20220806_210650-yaw62ovw/files/model-best)... Done. 0.9s\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r529/529 [==============================] - 99s 187ms/step - loss: 0.7960 - accuracy: 0.6287 - Sensitivity_Mild: 0.9008 - Sensitivity_Moderate: 0.7517 - Sensitivity_Severe: 0.9403 - val_loss: 0.6766 - val_accuracy: 0.7078 - val_Sensitivity_Mild: 0.9424 - val_Sensitivity_Moderate: 0.8705 - val_Sensitivity_Severe: 0.9438\n","Epoch 9/19\n","529/529 [==============================] - 89s 168ms/step - loss: 0.7356 - accuracy: 0.6544 - Sensitivity_Mild: 0.9329 - Sensitivity_Moderate: 0.8075 - Sensitivity_Severe: 0.9688 - val_loss: 0.7645 - val_accuracy: 0.6325 - val_Sensitivity_Mild: 0.9547 - val_Sensitivity_Moderate: 0.8042 - val_Sensitivity_Severe: 0.9775\n","Epoch 10/19\n","529/529 [==============================] - 89s 168ms/step - loss: 0.7305 - accuracy: 0.6737 - Sensitivity_Mild: 0.9153 - Sensitivity_Moderate: 0.8392 - Sensitivity_Severe: 0.9801 - val_loss: 0.7951 - val_accuracy: 0.5572 - val_Sensitivity_Mild: 0.9547 - val_Sensitivity_Moderate: 0.7952 - val_Sensitivity_Severe: 0.9663\n","Epoch 11/19\n","529/529 [==============================] - 88s 167ms/step - loss: 0.7490 - accuracy: 0.6495 - Sensitivity_Mild: 0.9308 - Sensitivity_Moderate: 0.7970 - Sensitivity_Severe: 0.9688 - val_loss: 0.8475 - val_accuracy: 0.5828 - val_Sensitivity_Mild: 0.9342 - val_Sensitivity_Moderate: 0.7078 - val_Sensitivity_Severe: 0.9551\n","Epoch 12/19\n","529/529 [==============================] - 88s 167ms/step - loss: 0.8093 - accuracy: 0.6091 - Sensitivity_Mild: 0.8853 - Sensitivity_Moderate: 0.7245 - Sensitivity_Severe: 0.9347 - val_loss: 0.7231 - val_accuracy: 0.6355 - val_Sensitivity_Mild: 0.9342 - val_Sensitivity_Moderate: 0.8765 - val_Sensitivity_Severe: 0.9663\n","Epoch 13/19\n","529/529 [==============================] - 88s 167ms/step - loss: 0.7581 - accuracy: 0.6405 - Sensitivity_Mild: 0.9091 - Sensitivity_Moderate: 0.7804 - Sensitivity_Severe: 0.9602 - val_loss: 0.7030 - val_accuracy: 0.6762 - val_Sensitivity_Mild: 0.9753 - val_Sensitivity_Moderate: 0.9006 - val_Sensitivity_Severe: 0.9326\n","Epoch 14/19\n","529/529 [==============================] - 88s 167ms/step - loss: 0.7250 - accuracy: 0.6722 - Sensitivity_Mild: 0.9246 - Sensitivity_Moderate: 0.8377 - Sensitivity_Severe: 0.9602 - val_loss: 1.2359 - val_accuracy: 0.4895 - val_Sensitivity_Mild: 0.9588 - val_Sensitivity_Moderate: 0.5422 - val_Sensitivity_Severe: 0.9775\n","Epoch 15/19\n","529/529 [==============================] - 88s 166ms/step - loss: 0.7374 - accuracy: 0.6503 - Sensitivity_Mild: 0.9194 - Sensitivity_Moderate: 0.7977 - Sensitivity_Severe: 0.9602 - val_loss: 0.6986 - val_accuracy: 0.6807 - val_Sensitivity_Mild: 0.9506 - val_Sensitivity_Moderate: 0.8645 - val_Sensitivity_Severe: 0.9775\n","Epoch 16/19\n","529/529 [==============================] - 88s 167ms/step - loss: 0.7408 - accuracy: 0.6612 - Sensitivity_Mild: 0.9360 - Sensitivity_Moderate: 0.8083 - Sensitivity_Severe: 0.9659 - val_loss: 0.8104 - val_accuracy: 0.6220 - val_Sensitivity_Mild: 0.9506 - val_Sensitivity_Moderate: 0.8976 - val_Sensitivity_Severe: 0.9213\n","Epoch 17/19\n","529/529 [==============================] - 88s 167ms/step - loss: 0.7155 - accuracy: 0.6681 - Sensitivity_Mild: 0.9442 - Sensitivity_Moderate: 0.8347 - Sensitivity_Severe: 0.9659 - val_loss: 0.6900 - val_accuracy: 0.6702 - val_Sensitivity_Mild: 0.9259 - val_Sensitivity_Moderate: 0.8404 - val_Sensitivity_Severe: 0.9775\n","Epoch 18/19\n","529/529 [==============================] - 88s 167ms/step - loss: 0.7044 - accuracy: 0.6820 - Sensitivity_Mild: 0.9349 - Sensitivity_Moderate: 0.8415 - Sensitivity_Severe: 0.9773 - val_loss: 0.7175 - val_accuracy: 0.6566 - val_Sensitivity_Mild: 0.9630 - val_Sensitivity_Moderate: 0.8735 - val_Sensitivity_Severe: 0.9551\n","Epoch 19/19\n","529/529 [==============================] - 88s 167ms/step - loss: 0.6895 - accuracy: 0.6866 - Sensitivity_Mild: 0.9494 - Sensitivity_Moderate: 0.8528 - Sensitivity_Severe: 0.9716 - val_loss: 0.6865 - val_accuracy: 0.7018 - val_Sensitivity_Mild: 0.9671 - val_Sensitivity_Moderate: 0.8735 - val_Sensitivity_Severe: 0.9663\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='597.729 MB of 597.729 MB uploaded (0.234 MB deduped)\\r'), FloatProgress(value=1.0,…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f84e4bf184c44e99a5196b1d9bfda5b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Sensitivity_Mild</td><td>▁▅▆▇▇▇▇▇█▇█▇▇█▇████</td></tr><tr><td>Sensitivity_Moderate</td><td>▁▃▅▆▆▆▆▆▇█▇▆▇█▇▇███</td></tr><tr><td>Sensitivity_Severe</td><td>▁▅▇▇▇▇▇▇███▇███████</td></tr><tr><td>accuracy</td><td>▁▄▅▆▆▆▆▆▇█▇▆▇█▇▇▇██</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▂▂▂▃▂▂▂▂▁▁▁</td></tr><tr><td>val_Sensitivity_Mild</td><td>▁▇▇▇▆▆▄▇▇▇▆▆█▇▇▇▆██</td></tr><tr><td>val_Sensitivity_Moderate</td><td>▄▂█▇▇▇▆▇▆▆▄██▁▇█▇▇▇</td></tr><tr><td>val_Sensitivity_Severe</td><td>▁▃▆▅▆█▄▅▇▆▆▆▄▇▇▃▇▆▆</td></tr><tr><td>val_accuracy</td><td>▁▃█▇▇▅▅█▆▄▄▆▇▂▇▆▇▇█</td></tr><tr><td>val_loss</td><td>▅▄▂▂▂▂▃▁▂▂▃▂▁█▁▃▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>GFLOPs</td><td>1.95803</td></tr><tr><td>Sensitivity_Mild</td><td>0.94938</td></tr><tr><td>Sensitivity_Moderate</td><td>0.85283</td></tr><tr><td>Sensitivity_Severe</td><td>0.97159</td></tr><tr><td>accuracy</td><td>0.68658</td></tr><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.67665</td></tr><tr><td>epoch</td><td>18</td></tr><tr><td>loss</td><td>0.68949</td></tr><tr><td>val_Sensitivity_Mild</td><td>0.96708</td></tr><tr><td>val_Sensitivity_Moderate</td><td>0.87349</td></tr><tr><td>val_Sensitivity_Severe</td><td>0.96629</td></tr><tr><td>val_accuracy</td><td>0.70181</td></tr><tr><td>val_loss</td><td>0.68652</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Synced <strong style=\"color:#cdcd00\">comic-sweep-9</strong>: <a href=\"https://wandb.ai/redet/final-project-results/runs/yaw62ovw\" target=\"_blank\">https://wandb.ai/redet/final-project-results/runs/yaw62ovw</a><br/>Synced 5 W&B file(s), 1 media file(s), 15 artifact file(s) and 2 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20220806_210650-yaw62ovw/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0ij6xjsh with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 5\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.09141889428250558\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5.568876827080684e-05\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: SGD\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.13.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/sample_data/wandb/run-20220806_213651-0ij6xjsh</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/redet/final-project-results/runs/0ij6xjsh\" target=\"_blank\">dark-sweep-10</a></strong> to <a href=\"https://wandb.ai/redet/final-project-results\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/redet/final-project-results/sweeps/3pmd91nk\" target=\"_blank\">https://wandb.ai/redet/final-project-results/sweeps/3pmd91nk</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":[">>>> Load pretrained from: /root/.keras/models/coatnet0_224_imagenet.h5\n","WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (512, 3). Received saved weight with shape (768, 1000)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (512, 3). Received saved weight with shape (768, 1000)\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (3,). Received saved weight with shape (1000,)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (3,). Received saved weight with shape (1000,)\n"]},{"output_type":"stream","name":"stdout","text":[">>>> Reload mismatched weights: 224 -> (244, 244)\n",">>>> Reload layer: stack_3_block_1_mhsa_pos_emb\n",">>>> Reload layer: stack_3_block_2_mhsa_pos_emb\n",">>>> Reload layer: stack_4_block_1_mhsa_pos_emb\n","Epoch 1/30\n","529/529 [==============================] - ETA: 0s - loss: 0.9876 - accuracy: 0.4851 - Sensitivity_Mild: 0.6209 - Sensitivity_Moderate: 0.5230 - Sensitivity_Severe: 0.6278"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `_wrapped_model` contains input name(s) bettiCurve with unsupported characters which will be renamed to betticurve in the SavedModel.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best)... Done. 0.5s\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r529/529 [==============================] - 96s 182ms/step - loss: 0.9876 - accuracy: 0.4851 - Sensitivity_Mild: 0.6209 - Sensitivity_Moderate: 0.5230 - Sensitivity_Severe: 0.6278 - val_loss: 0.9618 - val_accuracy: 0.5030 - val_Sensitivity_Mild: 0.7243 - val_Sensitivity_Moderate: 0.6325 - val_Sensitivity_Severe: 0.7191\n","Epoch 2/30\n","529/529 [==============================] - ETA: 0s - loss: 0.9388 - accuracy: 0.5361 - Sensitivity_Mild: 0.8171 - Sensitivity_Moderate: 0.6415 - Sensitivity_Severe: 0.7812"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `_wrapped_model` contains input name(s) bettiCurve with unsupported characters which will be renamed to betticurve in the SavedModel.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best)... Done. 0.3s\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r529/529 [==============================] - 96s 182ms/step - loss: 0.9388 - accuracy: 0.5361 - Sensitivity_Mild: 0.8171 - Sensitivity_Moderate: 0.6415 - Sensitivity_Severe: 0.7812 - val_loss: 0.9236 - val_accuracy: 0.5693 - val_Sensitivity_Mild: 0.9383 - val_Sensitivity_Moderate: 0.7470 - val_Sensitivity_Severe: 0.9213\n","Epoch 3/30\n","529/529 [==============================] - ETA: 0s - loss: 0.8876 - accuracy: 0.6015 - Sensitivity_Mild: 0.9081 - Sensitivity_Moderate: 0.7494 - Sensitivity_Severe: 0.9034"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `_wrapped_model` contains input name(s) bettiCurve with unsupported characters which will be renamed to betticurve in the SavedModel.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best)... Done. 0.3s\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r529/529 [==============================] - 96s 182ms/step - loss: 0.8876 - accuracy: 0.6015 - Sensitivity_Mild: 0.9081 - Sensitivity_Moderate: 0.7494 - Sensitivity_Severe: 0.9034 - val_loss: 0.8806 - val_accuracy: 0.6431 - val_Sensitivity_Mild: 0.9300 - val_Sensitivity_Moderate: 0.7530 - val_Sensitivity_Severe: 0.9663\n","Epoch 4/30\n","529/529 [==============================] - ETA: 0s - loss: 0.8286 - accuracy: 0.6431 - Sensitivity_Mild: 0.9452 - Sensitivity_Moderate: 0.7842 - Sensitivity_Severe: 0.9347"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `_wrapped_model` contains input name(s) bettiCurve with unsupported characters which will be renamed to betticurve in the SavedModel.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best)... Done. 0.3s\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r529/529 [==============================] - 96s 182ms/step - loss: 0.8286 - accuracy: 0.6431 - Sensitivity_Mild: 0.9452 - Sensitivity_Moderate: 0.7842 - Sensitivity_Severe: 0.9347 - val_loss: 0.7840 - val_accuracy: 0.6807 - val_Sensitivity_Mild: 0.9506 - val_Sensitivity_Moderate: 0.8735 - val_Sensitivity_Severe: 0.9551\n","Epoch 5/30\n","529/529 [==============================] - ETA: 0s - loss: 0.7476 - accuracy: 0.6662 - Sensitivity_Mild: 0.9473 - Sensitivity_Moderate: 0.8249 - Sensitivity_Severe: 0.9830"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `_wrapped_model` contains input name(s) bettiCurve with unsupported characters which will be renamed to betticurve in the SavedModel.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best)... Done. 0.3s\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r529/529 [==============================] - 96s 182ms/step - loss: 0.7476 - accuracy: 0.6662 - Sensitivity_Mild: 0.9473 - Sensitivity_Moderate: 0.8249 - Sensitivity_Severe: 0.9830 - val_loss: 0.7065 - val_accuracy: 0.7033 - val_Sensitivity_Mild: 0.9753 - val_Sensitivity_Moderate: 0.8916 - val_Sensitivity_Severe: 0.9888\n","Epoch 6/30\n","529/529 [==============================] - ETA: 0s - loss: 0.6908 - accuracy: 0.6851 - Sensitivity_Mild: 0.9597 - Sensitivity_Moderate: 0.8574 - Sensitivity_Severe: 0.9744"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `_wrapped_model` contains input name(s) bettiCurve with unsupported characters which will be renamed to betticurve in the SavedModel.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best)... Done. 0.3s\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r529/529 [==============================] - 97s 183ms/step - loss: 0.6908 - accuracy: 0.6851 - Sensitivity_Mild: 0.9597 - Sensitivity_Moderate: 0.8574 - Sensitivity_Severe: 0.9744 - val_loss: 0.6593 - val_accuracy: 0.7123 - val_Sensitivity_Mild: 0.9712 - val_Sensitivity_Moderate: 0.9006 - val_Sensitivity_Severe: 0.9888\n","Epoch 7/30\n","529/529 [==============================] - 88s 166ms/step - loss: 0.6489 - accuracy: 0.6930 - Sensitivity_Mild: 0.9700 - Sensitivity_Moderate: 0.8755 - Sensitivity_Severe: 0.9773 - val_loss: 0.6687 - val_accuracy: 0.7078 - val_Sensitivity_Mild: 0.9712 - val_Sensitivity_Moderate: 0.8946 - val_Sensitivity_Severe: 0.9775\n","Epoch 8/30\n","529/529 [==============================] - ETA: 0s - loss: 0.6199 - accuracy: 0.7248 - Sensitivity_Mild: 0.9711 - Sensitivity_Moderate: 0.8936 - Sensitivity_Severe: 0.9716"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `_wrapped_model` contains input name(s) bettiCurve with unsupported characters which will be renamed to betticurve in the SavedModel.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best)... Done. 0.3s\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r529/529 [==============================] - 96s 182ms/step - loss: 0.6199 - accuracy: 0.7248 - Sensitivity_Mild: 0.9711 - Sensitivity_Moderate: 0.8936 - Sensitivity_Severe: 0.9716 - val_loss: 0.6141 - val_accuracy: 0.7229 - val_Sensitivity_Mild: 0.9794 - val_Sensitivity_Moderate: 0.9096 - val_Sensitivity_Severe: 1.0000\n","Epoch 9/30\n","529/529 [==============================] - 88s 166ms/step - loss: 0.6089 - accuracy: 0.7308 - Sensitivity_Mild: 0.9731 - Sensitivity_Moderate: 0.9026 - Sensitivity_Severe: 0.9830 - val_loss: 0.6395 - val_accuracy: 0.7274 - val_Sensitivity_Mild: 0.9630 - val_Sensitivity_Moderate: 0.9217 - val_Sensitivity_Severe: 0.9663\n","Epoch 10/30\n","529/529 [==============================] - 88s 166ms/step - loss: 0.5872 - accuracy: 0.7459 - Sensitivity_Mild: 0.9711 - Sensitivity_Moderate: 0.9072 - Sensitivity_Severe: 0.9773 - val_loss: 0.9023 - val_accuracy: 0.5602 - val_Sensitivity_Mild: 0.9547 - val_Sensitivity_Moderate: 0.8946 - val_Sensitivity_Severe: 0.9438\n","Epoch 11/30\n","529/529 [==============================] - ETA: 0s - loss: 0.5705 - accuracy: 0.7550 - Sensitivity_Mild: 0.9752 - Sensitivity_Moderate: 0.9268 - Sensitivity_Severe: 0.9801"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `_wrapped_model` contains input name(s) bettiCurve with unsupported characters which will be renamed to betticurve in the SavedModel.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best)... Done. 0.3s\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r529/529 [==============================] - 96s 182ms/step - loss: 0.5705 - accuracy: 0.7550 - Sensitivity_Mild: 0.9752 - Sensitivity_Moderate: 0.9268 - Sensitivity_Severe: 0.9801 - val_loss: 0.6105 - val_accuracy: 0.7199 - val_Sensitivity_Mild: 0.9712 - val_Sensitivity_Moderate: 0.9127 - val_Sensitivity_Severe: 1.0000\n","Epoch 12/30\n","529/529 [==============================] - ETA: 0s - loss: 0.5643 - accuracy: 0.7573 - Sensitivity_Mild: 0.9711 - Sensitivity_Moderate: 0.9170 - Sensitivity_Severe: 0.9773"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `_wrapped_model` contains input name(s) bettiCurve with unsupported characters which will be renamed to betticurve in the SavedModel.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best)... Done. 0.3s\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r529/529 [==============================] - 96s 181ms/step - loss: 0.5643 - accuracy: 0.7573 - Sensitivity_Mild: 0.9711 - Sensitivity_Moderate: 0.9170 - Sensitivity_Severe: 0.9773 - val_loss: 0.5749 - val_accuracy: 0.7334 - val_Sensitivity_Mild: 0.9712 - val_Sensitivity_Moderate: 0.9277 - val_Sensitivity_Severe: 1.0000\n","Epoch 13/30\n","529/529 [==============================] - ETA: 0s - loss: 0.5497 - accuracy: 0.7603 - Sensitivity_Mild: 0.9773 - Sensitivity_Moderate: 0.9291 - Sensitivity_Severe: 0.9886"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `_wrapped_model` contains input name(s) bettiCurve with unsupported characters which will be renamed to betticurve in the SavedModel.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best/assets\n","\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/content/sample_data/wandb/run-20220806_213651-0ij6xjsh/files/model-best)... Done. 0.3s\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r529/529 [==============================] - 96s 182ms/step - loss: 0.5497 - accuracy: 0.7603 - Sensitivity_Mild: 0.9773 - Sensitivity_Moderate: 0.9291 - Sensitivity_Severe: 0.9886 - val_loss: 0.5664 - val_accuracy: 0.7425 - val_Sensitivity_Mild: 0.9753 - val_Sensitivity_Moderate: 0.9367 - val_Sensitivity_Severe: 0.9888\n","Epoch 14/30\n","529/529 [==============================] - 88s 166ms/step - loss: 0.5361 - accuracy: 0.7705 - Sensitivity_Mild: 0.9835 - Sensitivity_Moderate: 0.9381 - Sensitivity_Severe: 0.9744 - val_loss: 0.5845 - val_accuracy: 0.7455 - val_Sensitivity_Mild: 0.9712 - val_Sensitivity_Moderate: 0.9277 - val_Sensitivity_Severe: 0.9888\n","Epoch 15/30\n","529/529 [==============================] - 88s 166ms/step - loss: 0.5280 - accuracy: 0.7788 - Sensitivity_Mild: 0.9773 - Sensitivity_Moderate: 0.9396 - Sensitivity_Severe: 0.9830 - val_loss: 0.6002 - val_accuracy: 0.7440 - val_Sensitivity_Mild: 0.9671 - val_Sensitivity_Moderate: 0.9277 - val_Sensitivity_Severe: 0.9888\n","Epoch 16/30\n","196/529 [==========>...................] - ETA: 49s - loss: 0.5334 - accuracy: 0.7888 - Sensitivity_Mild: 0.9773 - Sensitivity_Moderate: 0.9420 - Sensitivity_Severe: 0.9792"]}],"source":["import wandb\n","from wandb.keras import WandbCallback\n","import math\n","\n","sweep_config={\n","    'method':'bayes',\n","    'metric':{\n","        'name': 'val_accuracy',\n","        'goal': 'maximize'\n","  },\n","  'parameters': {\n","    'optimizer':{\n","        'values':['adam','SGD']\n","    },\n","    'dropout':{\n","        'min':0.0000000000000001,\n","        'max':1.0\n","    },\n","    'learning_rate':{          \n","        'min': 0.00001,\n","        'max': 0.001\n","    },\n","    \"epochs\": {\n","        'distribution':'int_uniform',\n","        'min':10,\n","        'max':30\n","    },\n","    \"batch_size\":{\n","      'distribution':'q_log_uniform_values',\n","      'q':1,\n","      'min':4,\n","      'max':8\n","    }\n","  }\n","}\n","os.chdir(\"/content/sample_data\")\n","\n","# sweep_id = wandb.sweep(sweep_config,project=\"final-project-results\",entity=\"redet\");\n","sweep_id = \"3pmd91nk\"\n","wandb.agent(sweep_id, function=train,project=\"final-project-results\",entity=\"redet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h0-S_1QST_vD"},"outputs":[],"source":["import gc\n","\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vuqmYeqKT_j3"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"xOQbiis2GH0y"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1ZWb1V7dA8TR2jidshwWOCiZdyEMGIzw_","timestamp":1659466165049}],"mount_file_id":"1w6YDn9fh5yyYdeI1tb50KadQMjw_oTa4","authorship_tag":"ABX9TyMzmvqXWHKGysSeeg9RgZ6h"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f84e4bf184c44e99a5196b1d9bfda5b9":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_e00aafc1ad1645e7bf90c7cc61f788fe","IPY_MODEL_717fd1544ab4453587d8b857ef82659b"],"layout":"IPY_MODEL_8116d8e817fd485bbc5c424fd3683a1f"}},"e00aafc1ad1645e7bf90c7cc61f788fe":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f02c4cb4893a4a7b95ba20009af8a86c","placeholder":"​","style":"IPY_MODEL_b616f0d79a74465d98a8d4cc420eafab","value":"597.783 MB of 597.783 MB uploaded (0.234 MB deduped)\r"}},"717fd1544ab4453587d8b857ef82659b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_202c074cdcd144f2b155323e10d16079","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f572ea4327a4ea6843e1c87331078e6","value":1}},"8116d8e817fd485bbc5c424fd3683a1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f02c4cb4893a4a7b95ba20009af8a86c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b616f0d79a74465d98a8d4cc420eafab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"202c074cdcd144f2b155323e10d16079":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f572ea4327a4ea6843e1c87331078e6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}